{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import spacy\n",
    "import visdom\n",
    "import numpy as np\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "nlp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "En esta fase vamos a cargar los datos, procesarlos y visualizarlos.\n",
    "\n",
    "Para la visualización vamos a utilizar Visdom para lo que no tenemos que olvidar lanzar el servidor de visualizaciones, ejecutando:\n",
    " \n",
    "``` \n",
    "python -m visdom.server\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones visualización\n",
    "\n",
    "Hemos creado algunas funciones de visualización para ayudarnos a la hora de procesar los datos y tomar decisiones en la configuración de nuestras 'features' \n",
    "y parámetros del modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_field(vocab, size, title='Distribution'):\n",
    "    counts = []\n",
    "    labels = []\n",
    "    for i in range(size):\n",
    "        word = vocab.itos[i]\n",
    "        freq = vocab.freqs[word]\n",
    "        counts.append(freq)\n",
    "        labels.append(word)\n",
    "    vis.bar(X=counts, opts=dict(rownames=labels, title=title)) #opts=dict(numbins=30)\n",
    "\n",
    "def visualize_correlation(examples, vocab_text, vocab_labels, size, type='stacked', title='Correlation'):\n",
    "    heats = [ [0] * len(vocab_labels.itos) for i in range(size)]\n",
    "    words = [vocab_text.itos[i] for i in range(size)]\n",
    "    classes = [vocab_labels.itos[i] for i in range(len(vocab_labels.itos))]\n",
    "    for example in examples:\n",
    "        label_id = vocab_labels.stoi[example.label]\n",
    "        text = example.text\n",
    "        for token in text:\n",
    "            i = vocab_text.stoi[token]\n",
    "            if(i < size):\n",
    "                heats[i][label_id] += 1\n",
    "    if(type == 'stacked'):\n",
    "        vis.bar(\n",
    "            X=np.array(heats),\n",
    "            opts=dict(\n",
    "                stacked=True,\n",
    "                title=title,\n",
    "                legend=classes,\n",
    "                rownames=words\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        vis.heatmap(\n",
    "            X=np.array(heats),\n",
    "            opts=dict(\n",
    "                columnnames=classes,\n",
    "                rownames=words,\n",
    "                title=title,\n",
    "                colormap='Electric'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de procesamiento de texto\n",
    "\n",
    "\n",
    "Un aspecto clave ante cualquier problema de PLN es la manera en la que procesamos nuestros datos en crudo. Lo primero que tenemos que hacer es definir la manera en la que separamos el texto en los simbolos que contiene. Para ello vamos a definir una función de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_with_filter(filter):\n",
    "    def tokenize(text):\n",
    "        return [t.lower_ for t in nlp(text) if filter(t)]\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos\n",
    "\n",
    "Vamos a definir los datos de entrenamiento y prueba, así como los campos que vamos a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/oeg/dev/recognai/data/datasets/cosetdata/'\n",
    "\n",
    "# Primero definimos los campos del dataset de entrenamiento\n",
    "f = (lambda t: not t.is_stop and t.is_alpha) # Examples: t.is_alpha, full documentation at:\n",
    "twitter_id = data.Field()\n",
    "text = data.Field(tokenize=tokenize_with_filter(f))\n",
    "label = data.Field(sequential=False) # preprocessing=(lambda s: int(s))\n",
    "\n",
    "train = data.TabularDataset(path= DATASET_PATH + 'coset-train.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', twitter_id), ('text', text),('label',label)],\n",
    "                            skip_header=True)\n",
    "\n",
    "dev = data.TabularDataset(path= DATASET_PATH + 'coset-dev.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', twitter_id), ('text', text),('label',label)],\n",
    "                            skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de vocabularios\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.build_vocab(train)\n",
    "label.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización\n",
    "\n",
    "Vamos a visualizar nuestros datos.\n",
    "\n",
    "Podemos visualizar:\n",
    "\n",
    "* la distribución de palabras del texto de entrada (e.g., tweets)\n",
    "* la distribución de las etiquetas o clases asignadas a cada texto de entrada\n",
    "* o la correlación entre clases y palabras en el texto (correlation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_correlation(train.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'stacked', \n",
    "                      title='Train: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_correlation(train.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'heatmap', \n",
    "                      title='Train: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_field(text.vocab, \n",
    "                500, \n",
    "                title='Train: Distribución de palabras')\n",
    "\n",
    "visualize_field(label.vocab, \n",
    "                len(label.vocab.itos), \n",
    "                title='Train: Distribución de clases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '2', '9', '1', '11', '10']\n"
     ]
    }
   ],
   "source": [
    "# Si queremos ver la distribución en el dev set, podemos volver a crear el vocabulario solo usando el dev set\n",
    "text.build_vocab(dev)\n",
    "label.build_vocab(dev)\n",
    "\n",
    "visualize_correlation(dev.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'stacked', \n",
    "                      title='Dev: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_correlation(dev.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'heatmap', \n",
    "                      title='Dev: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_field(text.vocab, 500, title='Dev: Distribución de palabras')\n",
    "\n",
    "visualize_field(label.vocab, len(label.vocab.itos), title='Dev: Distribución de clases')\n",
    "print(label.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Por ejemplo:\n",
    "![title](../img/data-analysis.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
