{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'es'\u001b[0m\n",
      "\n",
      "    Only loading the 'es' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "import spacy\n",
    "import visdom\n",
    "import numpy as np\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "nlp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "En esta fase vamos a cargar los datos, procesarlos y visualizarlos.\n",
    "\n",
    "Para la visualización vamos a utilizar Visdom para lo que no tenemos que olvidar lanzar el servidor de visualizaciones, ejecutando:\n",
    " \n",
    "``` \n",
    "python -m visdom.server\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones visualización\n",
    "\n",
    "Hemos creado algunas funciones de visualización para ayudarnos a la hora de procesar los datos y tomar decisiones en la configuración de nuestras 'features' \n",
    "y parámetros del modelo predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de procesamiento de texto\n",
    "\n",
    "\n",
    "Un aspecto clave ante cualquier problema de PLN es la manera en la que procesamos nuestros datos en crudo. Lo primero que tenemos que hacer es definir la manera en la que separamos el texto en los simbolos que contiene. Para ello vamos a definir una función de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_with_filter(filter):\n",
    "    def tokenize(text):\n",
    "        return [t.lower_ for t in nlp(text) if filter(t)]\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos y batches\n",
    "\n",
    "Un aspecto clave ante cualquier problema de PLN es la manera en la que procesamos nuestros datos en crudo. Lo primero que tenemos que hacer es definir la manera en la que separamos el texto en los simbolos que contiene. Para ello vamos a definir una función de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = 'data/'\n",
    "\n",
    "# Primero definimos los campos del dataset de entrenamiento\n",
    "f = (lambda t: not t.is_stop and t.is_alpha) # Examples: t.is_alpha, full documentation at: and t.is_alpha\n",
    "f_min_length = (lambda t: not t.is_stop and t.is_alpha and len(t.orth_)>=3)\n",
    "twitter_id = data.Field()\n",
    "TEXT = data.Field(tokenize=tokenize_with_filter(f_min_length))\n",
    "LABEL = data.Field(sequential=False) \n",
    "\n",
    "trainset = data.TabularDataset(path= DATASET_PATH + 'coset-train.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', None), ('text', TEXT),('label',LABEL)],\n",
    "                            skip_header=True)\n",
    "\n",
    "devset = data.TabularDataset(path= DATASET_PATH + 'coset-dev.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', None), ('text', TEXT),('label',LABEL)],\n",
    "                            skip_header=True)\n",
    "\n",
    "# Let's build the vocabs\n",
    "TEXT.build_vocab(devset, min_freq=1)\n",
    "LABEL.build_vocab(trainset, devset)\n",
    "# Input dimensions are defined by the len of the input vocab\n",
    "input_dim = len(TEXT.vocab)\n",
    "print(len(TEXT.vocab))\n",
    "\n",
    "hidden_size = 1000\n",
    "# Output dimensions are two: \n",
    "output_dim = len(LABEL.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Definición del modelo\n",
    "\n",
    "Now we have our data ready, let's start defining our neural network structure.\n",
    "The core neural network components in Pytorch are in the nn module. This modules include typical layers: Linear, RNNs, CNNs, etc. which can be combined to create a multilayer neural network.\n",
    "Every model we create extends the basid nn.Module, and implements at least two methods:\n",
    "init: Defines the core variables of our network.\n",
    "forward: Defines the \"forward\" pass. This is, the computation made by our network to transform the input data into a prediction output.\n",
    "Let's create our first model, a simple multiclassifier classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can start defining our predictive model. The first step is to define the 'architecture' of the model\n",
    "# and its main operations with the data that goes through the network.\n",
    "\n",
    "# The core neural network components of Pytorch belong to the nn module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's start with a very simple baseline model\n",
    "class BaseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_dim, batch_size=32, debug=None):\n",
    "        super(BaseClassifier, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_dim)\n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # The forward pass defines how the input data is processed by the network\n",
    "        # to make a prediction\n",
    "        embed = self.embed(input)\n",
    "        # This operation summarizes a 3D tensor 200x32x200 into a 32x200 matrix\n",
    "        out = F.max_pool1d(embed.transpose(0,2), input.size()[0]).squeeze().transpose(0,1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.dropout(out, training=self.training, p=0.7)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.dropout(out, training=self.training, p=0.7)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de entrenamiento\n",
    "Now that we have defined the architecture of our network, we can start defining our training process. Ideally, this training process should be independent of our model architecture. A very naive approach would be to define a function which receives a model instance. Let's do this:. But first, let's define an auxiliary method for showing progress (do not worry much about this method now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log(time, epoch, iterations, batch_idx, train_iter, loss, train_acc, dev_loss=None, dev_acc=None, lot=None):\n",
    "    header = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \\n'\n",
    "    dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))\n",
    "    log_template =     ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "    print(header)\n",
    "    if(dev_loss):\n",
    "        print(dev_log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], dev_loss, train_acc, dev_acc))\n",
    "       \n",
    "        vis.line(\n",
    "                X=torch.ones((1, 2)).cpu() * iterations,\n",
    "                Y=torch.Tensor([loss.data[0], dev_loss]).unsqueeze(0).cpu(),\n",
    "                win=lot[0],\n",
    "                update='append'\n",
    "            )\n",
    "        vis.line(\n",
    "                X=torch.ones((1, 2)).cpu() * iterations,\n",
    "                Y=torch.Tensor([train_acc, dev_acc]).unsqueeze(0).cpu(),\n",
    "                win=lot[1],\n",
    "                update='append'\n",
    "            )\n",
    "    else:\n",
    "        \n",
    "        print(log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], ' '*8, train_acc, ' '*12))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(model, batches, num_epochs=2, learning_rate = 0.001, log_every = 500, dev_every = 500, lot=None):\n",
    "    import time\n",
    "    train_iter, dev_iter = batches\n",
    "    # First we need to define our loss/objective function\n",
    "    # Cross Entropy Loss already applies softmax\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # And the optimizer (Gradient-descent methods)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0009)\n",
    "    from torch.autograd import Variable\n",
    "    # Now the code for training our network\n",
    "    iterations = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter.init_epoch()\n",
    "        n_correct, n_total, f1, total_loss = 0, 0, 0, 0\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch.text)\n",
    "            iterations += 1\n",
    "            n_correct += (torch.max(output, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "            n_total += batch.batch_size\n",
    "            f1 += f1_score(torch.max(output, 1)[1].view(batch.label.size()).data.numpy(), batch.label.data.numpy(), average='macro')  \n",
    "            #print(f1)\n",
    "            loss = criterion(output, batch.label)\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "       \n",
    "        train_acc = f1/len(train_iter)\n",
    "        \n",
    "        model.eval(); dev_iter.init_epoch()\n",
    "        n_dev_correct, n_dev_total, f1, dev_loss = 0, 0, 0, 0\n",
    "        for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "            answer = model(dev_batch.text)\n",
    "            n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "            f1 += f1_score(torch.max(answer, 1)[1].view(dev_batch.label.size()).data.numpy(), dev_batch.label.data.numpy(), average='macro')  \n",
    "            n_dev_total += dev_batch.batch_size\n",
    "            dev_loss += criterion(answer, dev_batch.label)\n",
    "            \n",
    "        #dev_acc = 100. * n_dev_correct / n_dev_total\n",
    "        dev_acc = f1/len(dev_iter)\n",
    "        log(time.time()-start, \n",
    "                        epoch, \n",
    "                        iterations, \n",
    "                        batch_idx, \n",
    "                        train_iter, \n",
    "                        loss, \n",
    "                        train_acc,\n",
    "                        dev_loss.data[0]/len(dev_iter),\n",
    "                        dev_acc,\n",
    "                        lot=lot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento\n",
    "\n",
    "We can finally use our method for training and try out different models. Let's start with our simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with batch size 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:18: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:35: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "     6     0       141   141/141       100% 1.154382 1.430169       0.1988       0.1413\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    12     1       282   141/141       100% 0.865644 1.301288       0.2121       0.2745\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    18     2       423   141/141       100% 1.464697 1.375715       0.3305       0.2654\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    24     3       564   141/141       100% 0.842214 1.133457       0.4077       0.4579\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    31     4       705   141/141       100% 0.605438 1.194426       0.4867       0.3946\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    38     5       846   141/141       100% 0.729902 1.214610       0.4994       0.4054\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    44     6       987   141/141       100% 0.961124 1.126556       0.4965       0.4552\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    51     7      1128   141/141       100% 0.428181 1.064901       0.5235       0.4538\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    57     8      1269   141/141       100% 1.193207 1.216416       0.5442       0.4574\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    67     9      1410   141/141       100% 0.203384 1.277099       0.5627       0.4163\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    80    10      1551   141/141       100% 1.256157 1.081175       0.5665       0.4737\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "    93    11      1692   141/141       100% 0.432403 1.195588       0.5627       0.4876\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   107    12      1833   141/141       100% 0.492408 1.101750       0.6326       0.4878\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   121    13      1974   141/141       100% 2.205378 1.296061       0.6070       0.4767\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   135    14      2115   141/141       100% 0.717885 1.481946       0.6170       0.4437\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   149    15      2256   141/141       100% 0.313272 1.247491       0.6329       0.5130\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   164    16      2397   141/141       100% 0.151684 1.330215       0.6624       0.4565\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   178    17      2538   141/141       100% 0.392476 1.196474       0.6671       0.5203\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   193    18      2679   141/141       100% 0.070201 1.462228       0.6744       0.4912\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   208    19      2820   141/141       100% 0.044859 1.443229       0.7031       0.5008\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   224    20      2961   141/141       100% 0.349134 1.463711       0.7268       0.5261\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   240    21      3102   141/141       100% 0.294178 1.459929       0.7069       0.4806\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   257    22      3243   141/141       100% 1.573264 1.422528       0.7352       0.5243\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   273    23      3384   141/141       100% 0.049966 1.499237       0.7362       0.5039\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   290    24      3525   141/141       100% 0.306764 1.548276       0.7731       0.4802\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   308    25      3666   141/141       100% 0.026990 1.726560       0.7736       0.4761\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   327    26      3807   141/141       100% 0.632025 1.593813       0.7649       0.4670\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   347    27      3948   141/141       100% 0.238933 1.599341       0.7662       0.5190\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   368    28      4089   141/141       100% 0.354663 2.046988       0.7941       0.4539\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   391    29      4230   141/141       100% 0.008405 1.675249       0.7821       0.4632\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   413    30      4371   141/141       100% 0.770154 1.705220       0.7957       0.5100\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   441    31      4512   141/141       100% 0.001954 1.702931       0.8009       0.4841\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   474    32      4653   141/141       100% 0.020246 1.772160       0.7901       0.4914\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   500    33      4794   141/141       100% 0.105285 1.709170       0.7843       0.4665\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   526    34      4935   141/141       100% 0.068426 1.624131       0.7824       0.4699\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   556    35      5076   141/141       100% 0.956927 1.515920       0.7932       0.5495\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   586    36      5217   141/141       100% 0.009635 1.601177       0.7965       0.4648\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "   609    37      5358   141/141       100% 1.519632 1.678880       0.7921       0.4847\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4a58b6461040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#from joblib import Parallel, delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-4a58b6461040>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(batch_size, lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Let's call the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlot_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-7c3dea24414e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batches, num_epochs, learning_rate, log_every, dev_every, lot)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def trainer(batch_size, lr):\n",
    "    model = BaseClassifier(input_dim, hidden_size, output_dim)\n",
    "    print('Starting training with batch size {}'.format(batch_size))\n",
    "    train_iter, dev_iter = data.BucketIterator.splits((trainset, devset),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  sort_key=lambda x: len(x.text),\n",
    "                                                  device=-1,\n",
    "                                                  shuffle=True,\n",
    "                                                  repeat=False)\n",
    "    lot = vis.line(\n",
    "            X=torch.zeros((1,)).cpu(),\n",
    "            Y=torch.zeros((1, 2)).cpu(),\n",
    "            opts=dict(\n",
    "                xlabel='Iteration',\n",
    "                ylabel='Loss',\n",
    "                title='LOSS - Batch.{}.LR.{}'.format(batch_size,lr),\n",
    "                legend=['Train Loss', 'Dev Loss']\n",
    "            )\n",
    "        )\n",
    "    lot_acc = vis.line(\n",
    "            X=torch.zeros((1,)).cpu(),\n",
    "            Y=torch.zeros((1, 2)).cpu(),\n",
    "            opts=dict(\n",
    "                xlabel='Iteration',\n",
    "                ylabel='F1-Macro',\n",
    "                title='F1-Macro - Batch.{}.LR.{}'.format(batch_size,lr),\n",
    "                legend=['Train F1', 'Dev F1']\n",
    "            )\n",
    "        )\n",
    "    # Let's call the training process\n",
    "    train(model, (train_iter, dev_iter),num_epochs=100, learning_rate = lr, lot=[lot, lot_acc]) \n",
    "\n",
    "\n",
    "for batch_size in range(16, 64, 8):\n",
    "    for lr in np.arange(0.0005, 0.001, 0.0002):\n",
    "        trainer(batch_size, lr)\n",
    "    \n",
    "#from joblib import Parallel, delayed\n",
    "#Parallel(n_jobs=1)(delayed(trainer)(i) for i in range(8, 32, 8))\n",
    "\n",
    "#trainer(32)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
