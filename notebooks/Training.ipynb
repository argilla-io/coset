{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import spacy\n",
    "import visdom\n",
    "import numpy as np\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "nlp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "En esta fase vamos a cargar los datos, procesarlos y visualizarlos.\n",
    "\n",
    "Para la visualización vamos a utilizar Visdom para lo que no tenemos que olvidar lanzar el servidor de visualizaciones, ejecutando:\n",
    " \n",
    "``` \n",
    "python -m visdom.server\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones visualización\n",
    "\n",
    "Hemos creado algunas funciones de visualización para ayudarnos a la hora de procesar los datos y tomar decisiones en la configuración de nuestras 'features' \n",
    "y parámetros del modelo predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de procesamiento de texto\n",
    "\n",
    "\n",
    "Un aspecto clave ante cualquier problema de PLN es la manera en la que procesamos nuestros datos en crudo. Lo primero que tenemos que hacer es definir la manera en la que separamos el texto en los simbolos que contiene. Para ello vamos a definir una función de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_with_filter(filter):\n",
    "    def tokenize(text):\n",
    "        return [t.lower_ for t in nlp(text) if filter(t)]\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos y batches\n",
    "\n",
    "Un aspecto clave ante cualquier problema de PLN es la manera en la que procesamos nuestros datos en crudo. Lo primero que tenemos que hacer es definir la manera en la que separamos el texto en los simbolos que contiene. Para ello vamos a definir una función de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/oeg/dev/recognai/data/datasets/cosetdata/'\n",
    "\n",
    "# Primero definimos los campos del dataset de entrenamiento\n",
    "f = (lambda t: not t.is_stop and t.is_alpha) # Examples: t.is_alpha, full documentation at: and t.is_alpha\n",
    "\n",
    "twitter_id = data.Field()\n",
    "TEXT = data.Field(tokenize=tokenize_with_filter(f))\n",
    "LABEL = data.Field(sequential=False) \n",
    "\n",
    "trainset = data.TabularDataset(path= DATASET_PATH + 'coset-train.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', None), ('text', TEXT),('label',LABEL)],\n",
    "                            skip_header=True)\n",
    "\n",
    "devset = data.TabularDataset(path= DATASET_PATH + 'coset-dev.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', None), ('text', TEXT),('label',LABEL)],\n",
    "                            skip_header=True)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iter, dev_iter = data.BucketIterator.splits((trainset, devset),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  sort_key=lambda x: len(x.text),\n",
    "                                                  device=-1,\n",
    "                                                  repeat=False)\n",
    "\n",
    "# Let's build the vocabs\n",
    "TEXT.build_vocab(trainset)\n",
    "LABEL.build_vocab(trainset)\n",
    "# Input dimensions are defined by the len of the input vocab\n",
    "input_dim = len(TEXT.vocab)\n",
    "\n",
    "hidden_size = 1000\n",
    "# Output dimensions are two: \n",
    "output_dim = len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Definición del modelo\n",
    "\n",
    "Now we have our data ready, let's start defining our neural network structure.\n",
    "The core neural network components in Pytorch are in the nn module. This modules include typical layers: Linear, RNNs, CNNs, etc. which can be combined to create a multilayer neural network.\n",
    "Every model we create extends the basid nn.Module, and implements at least two methods:\n",
    "init: Defines the core variables of our network.\n",
    "forward: Defines the \"forward\" pass. This is, the computation made by our network to transform the input data into a prediction output.\n",
    "Let's create our first model, a simple multiclassifier classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can start defining our predictive model. The first step is to define the 'architecture' of the model\n",
    "# and its main operations with the data that goes through the network.\n",
    "\n",
    "# The core neural network components of Pytorch belong to the nn module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's start with a very simple baseline model\n",
    "class BaseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_dim, batch_size=32, debug=None):\n",
    "        super(BaseClassifier, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        \n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # The forward pass defines how the input data is processed by the network\n",
    "        # to make a prediction\n",
    "        if (self.debug):\n",
    "            print(input)\n",
    "        embed = self.embed(input)\n",
    "        if (self.debug):\n",
    "            print(embed)\n",
    "        # This operation summarizes a 3D tensor 200x32x200 into a 32x200 matrix\n",
    "        out = F.max_pool1d(embed.transpose(0,2), input.size()[0]).squeeze().transpose(0,1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.dropout(out, training=self.training, p=0.7)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de entrenamiento\n",
    "Now that we have defined the architecture of our network, we can start defining our training process. Ideally, this training process should be independent of our model architecture. A very naive approach would be to define a function which receives a model instance. Let's do this:. But first, let's define an auxiliary method for showing progress (do not worry much about this method now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lot = vis.line(\n",
    "            X=torch.zeros((1,)).cpu(),\n",
    "            Y=torch.zeros((1, 2)).cpu(),\n",
    "            opts=dict(\n",
    "                xlabel='Iteration',\n",
    "                ylabel='Loss',\n",
    "                title='Loss',\n",
    "                legend=['Train Loss', 'Dev Loss']\n",
    "            )\n",
    "        )\n",
    "def log(time, epoch, iterations, batch_idx, train_iter, loss, train_acc, dev_loss=None, dev_acc=None):\n",
    "    header = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \\n'\n",
    "    dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))\n",
    "    log_template =     ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "    print(header)\n",
    "    if(dev_loss):\n",
    "        print(dev_log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], dev_loss, train_acc, dev_acc))\n",
    "       \n",
    "        vis.line(\n",
    "                X=torch.ones((1, 2)).cpu() * iterations,\n",
    "                Y=torch.Tensor([loss.data[0], dev_loss]).unsqueeze(0).cpu(),\n",
    "                win=lot,\n",
    "                update='append'\n",
    "            )\n",
    "    else:\n",
    "        \n",
    "        print(log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], ' '*8, train_acc, ' '*12))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, batches, num_epochs=2, learning_rate = 0.001, log_every = 100, dev_every = 100):\n",
    "    import time\n",
    "    train_iter, dev_iter = batches\n",
    "    \n",
    "    # First we need to define our loss/objective function\n",
    "    # Cross Entropy Loss already applies softmax\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # And the optimizer (Gradient-descent methods)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    from torch.autograd import Variable\n",
    "    # Now the code for training our network\n",
    "    iterations = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter.init_epoch()\n",
    "        n_correct, n_total = 0, 0\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch.text)\n",
    "            iterations += 1\n",
    "            n_correct += (torch.max(output, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "            n_total += batch.batch_size\n",
    "            train_acc = 100. * n_correct/n_total\n",
    "            loss = criterion(output, batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iterations % log_every == 0:\n",
    "                log(time.time()-start, \n",
    "                    epoch, \n",
    "                    iterations, \n",
    "                    batch_idx, \n",
    "                    train_iter, \n",
    "                    loss, \n",
    "                    train_acc)\n",
    "            if iterations % dev_every == 0:\n",
    "                model.eval(); dev_iter.init_epoch()\n",
    "                n_dev_correct, dev_loss_total, n_total_dev = 0, 0, 0\n",
    "                for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "                    answer = model(dev_batch.text)\n",
    "                    n_total_dev += batch.batch_size\n",
    "                    n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "                    dev_loss = criterion(answer, dev_batch.label)\n",
    "                dev_acc = 100. * n_dev_correct / n_total_dev\n",
    "                print(dev_loss_total)\n",
    "                log(time.time()-start, \n",
    "                        epoch, \n",
    "                        iterations, \n",
    "                        batch_idx, \n",
    "                        train_iter, \n",
    "                        loss, \n",
    "                        train_acc,\n",
    "                        dev_loss,\n",
    "                        dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento\n",
    "\n",
    "We can finally use our method for training and try out different models. Let's start with our simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:17: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n",
      "     4     1       100    29/71         41% 1.359897               29.9569             \n",
      "\n",
      "Variable containing:\n",
      " 11.0988\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:38: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Variable.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a74faf0ca9c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Let's call the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-0352bd72600c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batches, num_epochs, learning_rate, log_every, dev_every)\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0mdev_loss_total\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                         dev_acc)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a8363676ed2a>\u001b[0m in \u001b[0;36mlog\u001b[0;34m(time, epoch, iterations, batch_idx, train_iter, loss, train_acc, dev_loss, dev_acc)\u001b[0m\n\u001b[1;32m     17\u001b[0m         print(dev_log_template.format(time,\n\u001b[1;32m     18\u001b[0m                     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     100. * (1+batch_idx) / len(train_iter), loss.data[0], dev_loss, train_acc, dev_acc))\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         vis.line(\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Variable.__format__"
     ]
    }
   ],
   "source": [
    "model = BaseClassifier(input_dim, hidden_size, output_dim)\n",
    "\n",
    "\n",
    "# Let's call the training process\n",
    "train(model, (train_iter, dev_iter),num_epochs=1000, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
