{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import spacy\n",
    "import visdom\n",
    "import numpy as np\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "nlp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "En esta fase vamos a cargar los datos, procesarlos y visualizarlos.\n",
    "\n",
    "Para la visualización vamos a utilizar Visdom para lo que no tenemos que olvidar lanzar el servidor de visualizaciones, ejecutando:\n",
    " \n",
    "``` \n",
    "python -m visdom.server\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones visualización\n",
    "\n",
    "Hemos creado algunas funciones de visualización para ayudarnos a la hora de procesar los datos y tomar decisiones en la configuración de nuestras 'features' \n",
    "y parámetros del modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_field(vocab, size, title='Distribution'):\n",
    "    counts = []\n",
    "    labels = []\n",
    "    for i in range(size):\n",
    "        word = vocab.itos[i]\n",
    "        freq = vocab.freqs[word]\n",
    "        counts.append(freq)\n",
    "        labels.append(word)\n",
    "    vis.bar(X=counts, opts=dict(rownames=labels, title=title)) #opts=dict(numbins=30)\n",
    "\n",
    "def visualize_correlation(examples, vocab_text, vocab_labels, size, type='stacked', title='Correlation'):\n",
    "    heats = [ [0] * len(label.vocab.itos) for i in range(size)]\n",
    "    words = [vocab_text.itos[i] for i in range(size)]\n",
    "    classes = [vocab_labels.itos[i] for i in range(len(vocab_labels.itos))]\n",
    "    for example in examples:\n",
    "        label_id = vocab_labels.stoi[example.label]\n",
    "        text = example.text\n",
    "        for token in text:\n",
    "            i = vocab_text.stoi[token]\n",
    "            if(i < size):\n",
    "                heats[i][label_id] += 1\n",
    "    if(type == 'stacked'):\n",
    "        vis.bar(\n",
    "            X=np.array(heats),\n",
    "            opts=dict(\n",
    "                stacked=True,\n",
    "                title=title,\n",
    "                legend=classes,\n",
    "                rownames=words\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        vis.heatmap(\n",
    "            X=np.array(heats),\n",
    "            opts=dict(\n",
    "                columnnames=classes,\n",
    "                rownames=words,\n",
    "                title=title,\n",
    "                colormap='Electric'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de procesamiento de texto\n",
    "\n",
    "\n",
    "Un aspecto clave ante cualquier problema de PLN es la manera en la que procesamos nuestros datos en crudo. Lo primero que tenemos que hacer es definir la manera en la que separamos el texto en los simbolos que contiene. Para ello vamos a definir una función de tokenización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_with_filter(filter):\n",
    "    def tokenize(text):\n",
    "        return [t.lower_ for t in nlp(text) if filter(t)]\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos\n",
    "\n",
    "Vamos a definir los datos de entrenamiento y prueba, así como los campos que vamos a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/oeg/dev/recognai/data/datasets/cosetdata/'\n",
    "\n",
    "# Primero definimos los campos del dataset de entrenamiento\n",
    "f = (lambda t: not t.is_stop and t.is_alpha) # Examples: t.is_alpha, full documentation at:\n",
    "twitter_id = data.Field()\n",
    "text = data.Field(tokenize=tokenize_with_filter(f))\n",
    "label = data.Field(sequential=False) # preprocessing=(lambda s: int(s))\n",
    "\n",
    "train = data.TabularDataset(path= DATASET_PATH + 'coset-train.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', twitter_id), ('text', text),('label',label)],\n",
    "                            skip_header=True)\n",
    "\n",
    "dev = data.TabularDataset(path= DATASET_PATH + 'coset-dev.csv',\n",
    "                            format='csv',\n",
    "                            fields= [('id', twitter_id), ('text', text),('label',label)],\n",
    "                            skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de vocabularios\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.build_vocab(train)\n",
    "label.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización\n",
    "\n",
    "Vamos a visualizar nuestros datos.\n",
    "\n",
    "Podemos visualizar:\n",
    "\n",
    "* la distribución de palabras del texto de entrada (e.g., tweets)\n",
    "* la distribución de las etiquetas o clases asignadas a cada texto de entrada\n",
    "* o la correlación entre clases y palabras en el texto (correlation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_correlation(train.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'stacked', \n",
    "                      title='Train: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_correlation(train.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'heatmap', \n",
    "                      title='Train: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_field(text.vocab, \n",
    "                500, \n",
    "                title='Train: Distribución de palabras')\n",
    "\n",
    "visualize_field(label.vocab, \n",
    "                len(label.vocab.itos), \n",
    "                title='Train: Distribución de clases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Si queremos ver la distribución en el dev set, podemos volver a crear el vocabulario solo usando el dev set\n",
    "text.build_vocab(dev)\n",
    "label.build_vocab(dev)\n",
    "\n",
    "visualize_correlation(dev.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'stacked', \n",
    "                      title='Dev: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_correlation(dev.examples, \n",
    "                      text.vocab, \n",
    "                      label.vocab, \n",
    "                      50, \n",
    "                      'heatmap', \n",
    "                      title='Dev: Correlación entre clases y palabras')\n",
    "\n",
    "visualize_field(text.vocab, 500, title='Dev: Distribución de palabras')\n",
    "\n",
    "visualize_field(label.vocab, len(label.vocab.itos), title='Dev: Distribución de clases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Por ejemplo:\n",
    "![title](../img/data-analysis.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de los datasets de entrenamiento y prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-2fa676c69f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oeg/dev/recognai/forks/text/torchtext/data.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "text.build_vocab(train, dev)\n",
    "label.build_vocab(train, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, dev_iter = data.BucketIterator.splits((train, dev),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  sort_key=lambda x: len(x.text),\n",
    "                                                  device=-1,\n",
    "                                                  repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Definición del modelo\n",
    "\n",
    "Now we have our data ready, let's start defining our neural network structure.\n",
    "The core neural network components in Pytorch are in the nn module. This modules include typical layers: Linear, RNNs, CNNs, etc. which can be combined to create a multilayer neural network.\n",
    "Every model we create extends the basid nn.Module, and implements at least two methods:\n",
    "init: Defines the core variables of our network.\n",
    "forward: Defines the \"forward\" pass. This is, the computation made by our network to transform the input data into a prediction output.\n",
    "Let's create our first model, a simple multiclassifier classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can start defining our predictive model. The first step is to define the 'architecture' of the model\n",
    "# and its main operations with the data that goes through the network.\n",
    "\n",
    "# The core neural network components of Pytorch belong to the nn module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's start with a very simple baseline model\n",
    "class BaseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_dim, batch_size=32, debug=None):\n",
    "        super(BaseClassifier, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        \n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # The forward pass defines how the input data is processed by the network\n",
    "        # to make a prediction\n",
    "        if (self.debug):\n",
    "            print(input)\n",
    "        embed = self.embed(input)\n",
    "        if (self.debug):\n",
    "            print(embed)\n",
    "        # This operation summarizes a 3D tensor 200x32x200 into a 32x200 matrix\n",
    "        out = F.max_pool1d(embed.transpose(0,2), input.size()[0]).squeeze().transpose(0,1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de entrenamiento\n",
    "Now that we have defined the architecture of our network, we can start defining our training process. Ideally, this training process should be independent of our model architecture. A very naive approach would be to define a function which receives a model instance. Let's do this:. But first, let's define an auxiliary method for showing progress (do not worry much about this method now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log(time, epoch, iterations, batch_idx, train_iter, loss, train_acc, dev_loss=None, dev_acc=None):\n",
    "    header = '  Time Epoch Iteration Progress    (%Epoch)   Loss   Dev/Loss     Accuracy  Dev/Accuracy'\n",
    "    dev_log_template = ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:8.6f},{:12.4f},{:12.4f}'.split(','))\n",
    "    log_template =     ' '.join('{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{},{:12.4f},{}'.split(','))\n",
    "    print(header)\n",
    "    if(dev_loss):\n",
    "        print(dev_log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], dev_loss, train_acc, dev_acc))\n",
    "    else:\n",
    "        print(log_template.format(time,\n",
    "                    epoch, iterations, 1+batch_idx, len(train_iter),\n",
    "                    100. * (1+batch_idx) / len(train_iter), loss.data[0], ' '*8, train_acc, ' '*12))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, batches, num_epochs=2, learning_rate = 0.001):\n",
    "    import time\n",
    "    train_iter, dev_iter = batches\n",
    "    \n",
    "    # First we need to define our loss/objective function\n",
    "    # Cross Entropy Loss already applies softmax\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # And the optimizer (Gradient-descent methods)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    from torch.autograd import Variable\n",
    "    # Now the code for training our network\n",
    "    iterations = 0\n",
    "    log_every = 300\n",
    "    dev_every = 700\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter.init_epoch()\n",
    "        n_correct, n_total = 0, 0\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch.text)\n",
    "            iterations += 1\n",
    "            n_correct += (torch.max(output, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "            n_total += batch.batch_size\n",
    "            train_acc = 100. * n_correct/n_total\n",
    "            loss = criterion(output, batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iterations % log_every == 0:\n",
    "                log(time.time()-start, \n",
    "                    epoch, \n",
    "                    iterations, \n",
    "                    batch_idx, \n",
    "                    train_iter, \n",
    "                    loss, \n",
    "                    train_acc)\n",
    "            if iterations % dev_every == 0:\n",
    "                model.eval(); dev_iter.init_epoch()\n",
    "                n_dev_correct, dev_loss = 0, 0\n",
    "                for dev_batch_idx, dev_batch in enumerate(dev_iter):\n",
    "                    answer = model(dev_batch.text)\n",
    "                    n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()).data == dev_batch.label.data).sum()\n",
    "                    dev_loss = criterion(answer, dev_batch.label)\n",
    "                dev_acc = 100. * n_dev_correct / len(dev)\n",
    "                log(time.time()-start, \n",
    "                        epoch, \n",
    "                        iterations, \n",
    "                        batch_idx, \n",
    "                        train_iter, \n",
    "                        loss, \n",
    "                        train_acc,\n",
    "                        dev_loss.data[0],\n",
    "                        dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento\n",
    "\n",
    "We can finally use our method for training and try out different models. Let's start with our simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Field' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-967419677c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Let's call the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-20024f192ddb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batches, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mn_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oeg/dev/recognai/forks/text/torchtext/data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                 yield Batch(minibatch, self.dataset, self.device,\n\u001b[0;32m--> 585\u001b[0;31m                             self.train)\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oeg/dev/recognai/forks/text/torchtext/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device, train)\u001b[0m\n\u001b[1;32m    488\u001b[0m                     setattr(self, name, field.numericalize(\n\u001b[1;32m    489\u001b[0m                         \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                         device=device, train=train))\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oeg/dev/recognai/forks/text/torchtext/data.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device, train)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oeg/dev/recognai/forks/text/torchtext/data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/oeg/dev/recognai/forks/text/torchtext/data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# Input dimensions are defined by the len of the input vocab\n",
    "input_dim = len(text.vocab)\n",
    "hidden_size = 200\n",
    "# Output dimensions are two: 0 = negative, 1 = positive\n",
    "output_dim = len(label.vocab)\n",
    "\n",
    "model = BaseClassifier(input_dim, hidden_size, output_dim)\n",
    "\n",
    "# Let's call the training process\n",
    "train(model, (train_iter, dev_iter),num_epochs=4, learning_rate = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
